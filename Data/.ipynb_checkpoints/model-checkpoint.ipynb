{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\distributed\\config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n",
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "string.punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from copy import deepcopy as dp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from copy import deepcopy\n",
    "import multiprocessing\n",
    "from time import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('data_train.csv')\n",
    "df1_test=pd.read_csv('data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3831, 956)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1),len(df1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.550817e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>I belong to a financially comfortable (read: R...</td>\n",
       "      <td>t5_2qh1q</td>\n",
       "      <td>Randians, how are the economic classes divided...</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/ate4ax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.547180e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>I mean does the caste system group people by i...</td>\n",
       "      <td>t5_2qh1q</td>\n",
       "      <td>[QUESTION] Someone from the San Francisco Bay ...</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/aeri5f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.548307e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t5_2qh1q</td>\n",
       "      <td>Microsoft says Bing search obstructed in China...</td>\n",
       "      <td>http://www.worldnewsheadline.ooo/2019/01/micro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.547215e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t5_2qh1q</td>\n",
       "      <td>2019 Lok Sabha election is clash of belief sys...</td>\n",
       "      <td>http://www.worldnewsheadline.ooo/2019/01/2019-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.552294e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>&amp;amp;#x200B;</td>\n",
       "      <td>t5_2qh1q</td>\n",
       "      <td>Please help ._. (Long Post,jumbled emotions ,s...</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/azr454...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc  link_flair_text  num_comments  score  \\\n",
       "0  1.550817e+09                3             6     11   \n",
       "1  1.547180e+09                3            12      5   \n",
       "2  1.548307e+09                3             0      1   \n",
       "3  1.547215e+09                3             0      1   \n",
       "4  1.552294e+09                3             3      9   \n",
       "\n",
       "                                            selftext subreddit_id  \\\n",
       "0  I belong to a financially comfortable (read: R...     t5_2qh1q   \n",
       "1  I mean does the caste system group people by i...     t5_2qh1q   \n",
       "2                                                NaN     t5_2qh1q   \n",
       "3                                                NaN     t5_2qh1q   \n",
       "4                                       &amp;#x200B;     t5_2qh1q   \n",
       "\n",
       "                                               title  \\\n",
       "0  Randians, how are the economic classes divided...   \n",
       "1  [QUESTION] Someone from the San Francisco Bay ...   \n",
       "2  Microsoft says Bing search obstructed in China...   \n",
       "3  2019 Lok Sabha election is clash of belief sys...   \n",
       "4  Please help ._. (Long Post,jumbled emotions ,s...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.reddit.com/r/india/comments/ate4ax...  \n",
       "1  https://www.reddit.com/r/india/comments/aeri5f...  \n",
       "2  http://www.worldnewsheadline.ooo/2019/01/micro...  \n",
       "3  http://www.worldnewsheadline.ooo/2019/01/2019-...  \n",
       "4  https://www.reddit.com/r/india/comments/azr454...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=df1['title'].tolist()\n",
    "body=df1['selftext'].tolist()\n",
    "url=df1['url'].tolist()\n",
    "y=df1['link_flair_text'].tolist()\n",
    "title_test=df1_test['title'].tolist()\n",
    "body_test=df1_test['selftext'].tolist()\n",
    "url_test=df1_test['url'].tolist()\n",
    "y_test=df1_test['link_flair_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stopwords_removal(tokens):\n",
    "    stop_words = list(stopwords.words(\"english\"))\n",
    "    filtered_sentence = [w for w in tokens if not w in stop_words]              \n",
    "    final=' '.join(filtered_sentence)\n",
    "    return final\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    obj=str.maketrans('', '',string.punctuation)\n",
    "    answer=text.translate(obj)\n",
    "    return answer\n",
    "\n",
    "def simple_lemma(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered=[lemmatizer.lemmatize(word) for word in data.split()]\n",
    "    text = ' '.join(filtered)\n",
    "    return text\n",
    "\n",
    "def pre_process(y,flag):\n",
    "    x=deepcopy(y)\n",
    "    for i in range(len(x)):\n",
    "        if str([i])!='nan':\n",
    "            if flag:\n",
    "                x[i]=x[i].split('/')\n",
    "                x[i]=' '.join(x[i])\n",
    "            x[i]=simple_lemma(remove_punctuation(stopwords_removal(word_tokenize(str(x[i]).lower()))))\n",
    "        else:\n",
    "            x[i]=''\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1=[0 for _ in range(11)]\n",
    "c2=[0 for _ in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df1['link_flair_text'].tolist():\n",
    "    c1[i]+=1\n",
    "for i in df1_test['link_flair_text']:\n",
    "    c2[i]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([196, 500, 500, 500, 500, 500, 41, 344, 244, 406, 100],\n",
       " [49, 125, 125, 125, 125, 125, 10, 86, 61, 101, 24])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1,c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_clean=pre_process(title,False)\n",
    "body_clean=pre_process(body,False)\n",
    "url_clean=pre_process(url,True)\n",
    "title_clean_test=pre_process(title_test,False)\n",
    "body_clean_test=pre_process(body_test,False)\n",
    "url_clean_test=pre_process(url_test,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Randians, how are the economic classes divided in India and what are the differences in their lifestyles?',\n",
       " 'randians economic class divided india difference lifestyle')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title[0],title_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "\n",
      "\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "print(body[3])\n",
    "print(\"\\n\")\n",
    "print(body_clean[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier,X_train,y_train,X_test,y_test):\n",
    "    classifier.fit(X_train,y_train)\n",
    "    y_train_pred=classifier.predict(X_train)\n",
    "    y_test_pred=classifier.predict(X_test)\n",
    "    return accuracy_score(y_train,y_train_pred),accuracy_score(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test(X_train,X_test):\n",
    "    tfidf= TfidfVectorizer(max_features=2000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "    X1_train=tfidf.fit_transform(X_train)\n",
    "    X1_test=tfidf.transform(X_test)\n",
    "    \n",
    "    clf1=GridSearchCV(SVC(gamma='scale'),{'decision_function_shape':['ovo','ovr'],},cv=5,iid=False)\n",
    "    print(train(clf1,X1_train,y,X1_test,y_test))\n",
    "    \n",
    "    clf2=GridSearchCV(DecisionTreeClassifier(random_state=0),{'criterion':['gini','entropy']},cv=5,iid=False)\n",
    "    print(train(clf2,X1_train,y,X1_test,y_test))\n",
    "\n",
    "    clf3=GridSearchCV(LogisticRegression(random_state=0),{'solver':['newton-cg', 'lbfgs', 'saga'],'multi_class':['multinomial'],'max_iter':[500,1000,1500]},cv=5,iid=False)\n",
    "    print(train(clf3,X1_train,y,X1_test,y_test))\n",
    "\n",
    "    clf4=GridSearchCV(RandomForestClassifier(random_state=0),{'n_estimators':[10,50,100,150],'criterion':['gini','entropy']},cv=5,iid=False)\n",
    "    print(train(clf4,X1_train,y,X1_test,y_test))\n",
    "\n",
    "    clf5=GridSearchCV(xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42),{'max_depth':[2,3],'learning_rate':[0.1,0.15,0.2]},cv=5,iid=False)\n",
    "    print(train(clf5,X1_train,y,X1_test,y_test))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8919342208300705, 0.5010460251046025)\n",
      "(0.9527538501696685, 0.42782426778242677)\n",
      "(0.7227877838684417, 0.5104602510460251)\n"
     ]
    }
   ],
   "source": [
    "train_test(title_clean,title_clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test(url_clean,url_clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test(body_clean,body_clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti_bo=[]\n",
    "ti_bo_test=[]\n",
    "for i in range(len(title_clean)):\n",
    "    ti_bo.append(title_clean[i]+\" \"+body_clean[i])\n",
    "for i in range(len(title_clean_test)):\n",
    "    ti_bo_test.append(title_clean_test[i]+' '+body_clean_test[i])\n",
    "train_test(ti_bo,ti_bo_test)\n",
    "print(len(title_clean),len(title_clean_test),len(body_clean_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti_bo_ur=[]\n",
    "for i in range(len(title_clean)):\n",
    "    ti_bo_ur.append(title_clean[i]+' '+body_clean[i]+' '+url_clean[i])\n",
    "ti_bo_ur_test=[]\n",
    "for i in range(len(title_clean_test)):\n",
    "    ti_bo_ur_test.append(title_clean_test[i]+' '+body_clean_test[i]+' '+url_clean_test[i])\n",
    "train_test(ti_bo_ur,ti_bo_ur_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_ur=[]\n",
    "for i in range(len(body_clean)):\n",
    "    bo_ur.append(body_clean[i]+' '+url_clean[i])\n",
    "bo_ur_test=[]\n",
    "for i in range(len(body_clean_test)):\n",
    "    bo_ur_test.append(body_clean_test[i]+' '+url_clean_test[i])\n",
    "train_test(bo_ur,bo_ur_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti_ur=[]\n",
    "for i in range(len(title_clean)):\n",
    "    ti_ur.append(title_clean[i]+' '+url_clean[i])\n",
    "ti_ur_test=[]\n",
    "for i in range(len(title_clean_test)):\n",
    "    ti_ur_test.append(title_clean_test[i]+' '+url_clean_test[i])\n",
    "train_test(ti_ur,ti_ur_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=GridSearchCV(SVC(gamma='scale'),{'decision_function_shape':['ovo','ovr'],},cv=5,iid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf= TfidfVectorizer(max_features=2000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "X = tfidf.fit_transform(ti_bo_ur)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict(X)\n",
    "print(accuracy_score(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Model','wb') as f:\n",
    "    pickle.dump(clf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf','wb') as f:\n",
    "    pickle.dump(tfidf,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
