{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "string.punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from copy import deepcopy as dp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from copy import deepcopy\n",
    "import multiprocessing\n",
    "from time import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('data_train.csv')\n",
    "df1_test=pd.read_csv('data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3834, 956)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1),len(df1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.553629e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Hi, I'm 16yo from Kandivali, Mumbai. I play ac...</td>\n",
       "      <td>t5_2qh1q</td>\n",
       "      <td>Looking to make friends who are musicians!</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/b5u3or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.547942e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>29</td>\n",
       "      <td>Hi friends, I'm working on an essay on Indian ...</td>\n",
       "      <td>t5_2qh1q</td>\n",
       "      <td>What bothers you as an Indian?</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/ahrxc7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.550300e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>I'm an Indian and I can understand Hindi but c...</td>\n",
       "      <td>t5_2qh1q</td>\n",
       "      <td>What are the best guides to learn to speak Hindi?</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/ar6dvb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.546852e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>This post was inspired by another post at the ...</td>\n",
       "      <td>t5_2qh1q</td>\n",
       "      <td>Tips to become a good driver</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/adfxwr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.547511e+09</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>t5_2qh1q</td>\n",
       "      <td>Earning WhatsApp Group Join Link List</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/ag2869...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc  link_flair_text  num_comments  score  \\\n",
       "0  1.553629e+09                3             6      1   \n",
       "1  1.547942e+09                3            67     29   \n",
       "2  1.550300e+09                3             4      7   \n",
       "3  1.546852e+09                3            19     21   \n",
       "4  1.547511e+09                3             0      1   \n",
       "\n",
       "                                            selftext subreddit_id  \\\n",
       "0  Hi, I'm 16yo from Kandivali, Mumbai. I play ac...     t5_2qh1q   \n",
       "1  Hi friends, I'm working on an essay on Indian ...     t5_2qh1q   \n",
       "2  I'm an Indian and I can understand Hindi but c...     t5_2qh1q   \n",
       "3  This post was inspired by another post at the ...     t5_2qh1q   \n",
       "4                                          [removed]     t5_2qh1q   \n",
       "\n",
       "                                               title  \\\n",
       "0         Looking to make friends who are musicians!   \n",
       "1                     What bothers you as an Indian?   \n",
       "2  What are the best guides to learn to speak Hindi?   \n",
       "3                       Tips to become a good driver   \n",
       "4              Earning WhatsApp Group Join Link List   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.reddit.com/r/india/comments/b5u3or...  \n",
       "1  https://www.reddit.com/r/india/comments/ahrxc7...  \n",
       "2  https://www.reddit.com/r/india/comments/ar6dvb...  \n",
       "3  https://www.reddit.com/r/india/comments/adfxwr...  \n",
       "4  https://www.reddit.com/r/india/comments/ag2869...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=df1['title'].tolist()\n",
    "body=df1['selftext'].tolist()\n",
    "url=df1['url'].tolist()\n",
    "y=df1['link_flair_text'].tolist()\n",
    "title_test=df1_test['title'].tolist()\n",
    "body_test=df1_test['selftext'].tolist()\n",
    "url_test=df1_test['url'].tolist()\n",
    "y_test=df1_test['link_flair_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stopwords_removal(tokens):\n",
    "    stop_words = list(stopwords.words(\"english\"))\n",
    "    filtered_sentence = [w for w in tokens if not w in stop_words]              \n",
    "    final=' '.join(filtered_sentence)\n",
    "    return final\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    obj=str.maketrans('', '',string.punctuation)\n",
    "    answer=text.translate(obj)\n",
    "    return answer\n",
    "\n",
    "def simple_lemma(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered=[lemmatizer.lemmatize(word) for word in data.split()]\n",
    "    text = ' '.join(filtered)\n",
    "    return text\n",
    "\n",
    "def pre_process(y,flag):\n",
    "    x=deepcopy(y)\n",
    "    for i in range(len(x)):\n",
    "        if str([i])!='nan':\n",
    "            if flag:\n",
    "                x[i]=x[i].split('/')\n",
    "                x[i]=' '.join(x[i])\n",
    "            x[i]=simple_lemma(remove_punctuation(stopwords_removal(word_tokenize(str(x[i]).lower()))))\n",
    "        else:\n",
    "            x[i]=''\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1=[0 for _ in range(11)]\n",
    "c2=[0 for _ in range(11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df1['link_flair_text'].tolist():\n",
    "    c1[i]+=1\n",
    "for i in df1_test['link_flair_text']:\n",
    "    c2[i]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([199, 500, 500, 500, 500, 500, 41, 344, 244, 406, 100],\n",
       " [49, 125, 125, 125, 125, 125, 10, 86, 61, 101, 24])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1,c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_clean=pre_process(title,False)\n",
    "body_clean=pre_process(body,False)\n",
    "url_clean=pre_process(url,True)\n",
    "title_clean_test=pre_process(title_test,False)\n",
    "body_clean_test=pre_process(body_test,False)\n",
    "url_clean_test=pre_process(url_test,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Looking to make friends who are musicians!', 'looking make friend musician')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title[0],title_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This post was inspired by another post at the top today - https://www.reddit.com/r/india/comments/addzf5/why_does_everyone_have_their_highbeams_on/\n",
      "\n",
      "And this comment especially,\n",
      "\n",
      "&gt; Because no one taught them how high beams work or how pass lights work or how turn indicators work or...\n",
      "\n",
      "This is me. I wasn't taught well and I have a inherited a lot of bad driving habits that I now want to correct.\n",
      "\n",
      "Please help me become a responsible driver.\n",
      "\n",
      "\n",
      "post inspired another post top today http wwwredditcomrindiacommentsaddzf5whydoeseveryonehavetheirhighbeamson comment especially gt one taught high beam work pas light work turn indicator work nt taught well inherited lot bad driving habit want correct please help become responsible driver\n"
     ]
    }
   ],
   "source": [
    "print(body[3])\n",
    "print(\"\\n\")\n",
    "print(body_clean[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier,X_train,y_train,X_test,y_test):\n",
    "    classifier.fit(X_train,y_train)\n",
    "    y_train_pred=classifier.predict(X_train)\n",
    "    y_test_pred=classifier.predict(X_test)\n",
    "    return accuracy_score(y_train,y_train_pred),accuracy_score(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test(X_train,X_test):\n",
    "    tfidf= TfidfVectorizer(max_features=2000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "    X1_train=tfidf.fit_transform(X_train)\n",
    "    X1_test=tfidf.transform(X_test)\n",
    "    \n",
    "    clf1=GridSearchCV(SVC(gamma='scale'),{'decision_function_shape':['ovo','ovr'],},cv=5,iid=False)\n",
    "    print(train(clf1,X1_train,y,X1_test,y_test))\n",
    "    \n",
    "    clf2=GridSearchCV(DecisionTreeClassifier(random_state=0),{'criterion':['gini','entropy']},cv=5,iid=False)\n",
    "    print(train(clf2,X1_train,y,X1_test,y_test))\n",
    "\n",
    "    clf3=GridSearchCV(LogisticRegression(random_state=0),{'solver':['newton-cg', 'lbfgs', 'saga'],'multi_class':['multinomial'],'max_iter':[500,1000,1500]},cv=5,iid=False)\n",
    "    print(train(clf3,X1_train,y,X1_test,y_test))\n",
    "\n",
    "    clf4=GridSearchCV(RandomForestClassifier(random_state=0),{'n_estimators':[10,50,100,150],'criterion':['gini','entropy']},cv=5,iid=False)\n",
    "    print(train(clf4,X1_train,y,X1_test,y_test))\n",
    "\n",
    "    clf5=GridSearchCV(xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42),{'max_depth':[2,3],'learning_rate':[0.1,0.15,0.2]},cv=5,iid=False)\n",
    "    print(train(clf5,X1_train,y,X1_test,y_test))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8873239436619719, 0.4884937238493724)\n",
      "(0.9577464788732394, 0.3880753138075314)\n",
      "(0.724830464267084, 0.4884937238493724)\n",
      "(0.9577464788732394, 0.47384937238493724)\n",
      "(0.6674491392801252, 0.44037656903765693)\n"
     ]
    }
   ],
   "source": [
    "train_test(title_clean,title_clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4590505998956703, 0.3985355648535565)\n",
      "(0.4809598330725091, 0.3797071129707113)\n",
      "(0.4428794992175274, 0.39644351464435146)\n",
      "(0.4809598330725091, 0.3891213389121339)\n",
      "(0.45539906103286387, 0.39330543933054396)\n"
     ]
    }
   ],
   "source": [
    "train_test(url_clean,url_clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3075117370892019, 0.23326359832635984)\n",
      "(0.32785602503912364, 0.2395397489539749)\n",
      "(0.2540427751695357, 0.24476987447698745)\n",
      "(0.32785602503912364, 0.2510460251046025)\n",
      "(0.28351591027647366, 0.23744769874476987)\n"
     ]
    }
   ],
   "source": [
    "train_test(body_clean,body_clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9290558163797601, 0.5240585774058577)\n",
      "(0.9754825247782994, 0.37343096234309625)\n",
      "(0.7556077203964527, 0.5313807531380753)\n",
      "(0.9754825247782994, 0.49372384937238495)\n",
      "(0.7112676056338029, 0.49581589958158995)\n",
      "3834 956 956\n"
     ]
    }
   ],
   "source": [
    "ti_bo=[]\n",
    "ti_bo_test=[]\n",
    "for i in range(len(title_clean)):\n",
    "    ti_bo.append(title_clean[i]+\" \"+body_clean[i])\n",
    "for i in range(len(title_clean_test)):\n",
    "    ti_bo_test.append(title_clean_test[i]+' '+body_clean_test[i])\n",
    "train_test(ti_bo,ti_bo_test)\n",
    "print(len(title_clean),len(title_clean_test),len(body_clean_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9264475743348983, 0.5575313807531381)\n",
      "(0.985133020344288, 0.42887029288702927)\n",
      "(0.7496087636932708, 0.5700836820083682)\n",
      "(0.985133020344288, 0.5449790794979079)\n",
      "(0.7378716744913928, 0.5753138075313807)\n"
     ]
    }
   ],
   "source": [
    "ti_bo_ur=[]\n",
    "for i in range(len(title_clean)):\n",
    "    ti_bo_ur.append(title_clean[i]+' '+body_clean[i]+' '+url_clean[i])\n",
    "ti_bo_ur_test=[]\n",
    "for i in range(len(title_clean_test)):\n",
    "    ti_bo_ur_test.append(title_clean_test[i]+' '+body_clean_test[i]+' '+url_clean_test[i])\n",
    "train_test(ti_bo_ur,ti_bo_ur_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5878977569118414, 0.4299163179916318)\n",
      "(0.634585289514867, 0.399581589958159)\n",
      "(0.5179968701095462, 0.44142259414225943)\n",
      "(0.634585289514867, 0.4361924686192469)\n",
      "(0.548774126238915, 0.4320083682008368)\n"
     ]
    }
   ],
   "source": [
    "bo_ur=[]\n",
    "for i in range(len(body_clean)):\n",
    "    bo_ur.append(body_clean[i]+' '+url_clean[i])\n",
    "bo_ur_test=[]\n",
    "for i in range(len(body_clean_test)):\n",
    "    bo_ur_test.append(body_clean_test[i]+' '+url_clean_test[i])\n",
    "train_test(bo_ur,bo_ur_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8951486697965572, 0.5428870292887029)\n",
      "(0.9778299426186751, 0.4581589958158996)\n",
      "(0.7516953573291602, 0.5658995815899581)\n",
      "(0.9778299426186751, 0.5219665271966527)\n",
      "(0.6987480438184663, 0.5313807531380753)\n"
     ]
    }
   ],
   "source": [
    "ti_ur=[]\n",
    "for i in range(len(title_clean)):\n",
    "    ti_ur.append(title_clean[i]+' '+url_clean[i])\n",
    "ti_ur_test=[]\n",
    "for i in range(len(title_clean_test)):\n",
    "    ti_ur_test.append(title_clean_test[i]+' '+url_clean_test[i])\n",
    "train_test(ti_ur,ti_ur_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=GridSearchCV(SVC(gamma='scale'),{'decision_function_shape':['ovo','ovr'],},cv=5,iid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf= TfidfVectorizer(max_features=2000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "X = tfidf.fit_transform(ti_bo_ur)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=False, n_jobs=None,\n",
       "       param_grid={'decision_function_shape': ['ovo', 'ovr']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9264475743348983\n"
     ]
    }
   ],
   "source": [
    "y_pred=clf.predict(X)\n",
    "print(accuracy_score(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Model','wb') as f:\n",
    "    pickle.dump(clf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf','wb') as f:\n",
    "    pickle.dump(tfidf,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
